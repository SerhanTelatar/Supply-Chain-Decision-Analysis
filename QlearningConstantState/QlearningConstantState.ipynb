{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from gym import Env, spaces\n",
    "import random\n",
    "import itertools\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "potential_demand = np.arange(50, 151)  # Possible demand values from 50 to 150\n",
    "stock_list = np.arange(50, 151)       # Possible stock values from 50 to 150\n",
    "expected_sales = {}                   # Dictionary to store expected sales\n",
    "\n",
    "\n",
    "for stock in stock_list:\n",
    "    cumulative_expected_sales = 0\n",
    "    for demand_realization in potential_demand:\n",
    "        sales = min(demand_realization, stock)\n",
    "        cumulative_expected_sales += sales\n",
    "\n",
    "    # Calculate the average sales and store it in the dictionary\n",
    "    expected_sales[stock] = cumulative_expected_sales / len(potential_demand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SupplyChainEnv(Env):\n",
    "    def __init__(self, contract_type=\"wholesale\", expected_sales=None, maxRounds=1000):\n",
    "        super(SupplyChainEnv, self).__init__()\n",
    "        self.contract_type = contract_type\n",
    "        self.max_stock = 150\n",
    "        self.min_w = 3\n",
    "        self.max_price = 12\n",
    "        self.max_rounds = maxRounds\n",
    "        self.current_round = 0\n",
    "        self.demand = 0\n",
    "        self.expected_sales = expected_sales\n",
    "\n",
    "        if self.contract_type == \"wholesale\":\n",
    "            self.manufacturer_action_space = spaces.Discrete(\n",
    "                self.max_price - self.min_w + 1\n",
    "            )\n",
    "            self.retailer_action_space = spaces.Discrete(5)\n",
    "        elif self.contract_type == \"buyback\":\n",
    "            self.manufacturer_action_space = spaces.MultiDiscrete(\n",
    "                [self.max_price - self.min_w + 1, self.max_price + 1]\n",
    "            )\n",
    "            self.retailer_action_space = spaces.Discrete(5)\n",
    "        elif self.contract_type == \"revenue-sharing\":\n",
    "            self.manufacturer_action_space = spaces.MultiDiscrete(\n",
    "                [self.max_price - self.min_w + 1, self.max_price + 1]\n",
    "            )\n",
    "            self.retailer_action_space = spaces.Discrete(5)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid contract type.\")\n",
    "\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=0, high=150, shape=(3,), dtype=np.float32\n",
    "        )\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = np.array([0, 0, 0])\n",
    "        self.current_round = 0\n",
    "        self.demand = 0\n",
    "        return self.state\n",
    "\n",
    "    def manufacturer_step(self, action):\n",
    "        if self.contract_type == \"wholesale\":\n",
    "            w = action + self.min_w\n",
    "            b = 0\n",
    "            r = 0\n",
    "        elif self.contract_type == \"buyback\":\n",
    "            w = action[0] + self.min_w\n",
    "            b = action[1]\n",
    "            r = 0\n",
    "        elif self.contract_type == \"revenue-sharing\":\n",
    "            w = action[0] + self.min_w\n",
    "            r = action[1]\n",
    "            b = 0\n",
    "\n",
    "        self.state = np.array([w, b, r])\n",
    "        return self.state\n",
    "\n",
    "    def get_optimal_stock(self):\n",
    "        w, b, r = self.state\n",
    "        if self.contract_type == \"wholesale\":\n",
    "            optimal_stock = 100 * ((12 - w) / 12) + 50\n",
    "        elif self.contract_type == \"buyback\":\n",
    "            if b == 12:\n",
    "                optimal_stock = 150\n",
    "            else:\n",
    "                optimal_stock = 100 * ((12 - w) / (12 - b)) + 50\n",
    "        elif self.contract_type == \"revenue-sharing\":\n",
    "            if r == 12:\n",
    "                optimal_stock = 0\n",
    "            else:\n",
    "                optimal_stock = 100 * ((12 - w - r) / (12 - r)) + 50\n",
    "        else:\n",
    "            optimal_stock = 100\n",
    "        return optimal_stock\n",
    "\n",
    "    def retailer_step(self, action):\n",
    "        w, b, r = self.state\n",
    "\n",
    "        if self.contract_type == \"wholesale\":\n",
    "            optimal_stock = 100 * ((12 - w) / 12) + 50\n",
    "        elif self.contract_type == \"buyback\":\n",
    "            if b == 12:\n",
    "                optimal_stock = 150\n",
    "            else:\n",
    "                optimal_stock = 100 * ((12 - w) / (12 - b)) + 50\n",
    "        elif self.contract_type == \"revenue-sharing\":\n",
    "            if r == 12:\n",
    "                optimal_stock = 0\n",
    "            else:\n",
    "                optimal_stock = 100 * ((12 - w - r) / (12 - r)) + 50\n",
    "        else:\n",
    "            optimal_stock = 100\n",
    "\n",
    "        if action == 0:\n",
    "            Q = optimal_stock * 0.8\n",
    "        elif action == 1:\n",
    "            Q = optimal_stock * 0.9\n",
    "        elif action == 2:\n",
    "            Q = optimal_stock\n",
    "        elif action == 3:\n",
    "            Q = optimal_stock * 1.1\n",
    "        elif action == 4:\n",
    "            Q = optimal_stock * 1.2\n",
    "        else:\n",
    "            Q = optimal_stock\n",
    "\n",
    "        Q = int(round(Q))\n",
    "        Q = max(0, min(Q, self.max_stock))\n",
    "\n",
    "        sales = self.expected_sales.get(Q, 0)\n",
    "        leftovers = Q - sales\n",
    "        c = 3\n",
    "        p = 12\n",
    "\n",
    "        if self.contract_type == \"wholesale\":\n",
    "            retailer_profit = p * sales - w * Q\n",
    "            manufacturer_profit = (w - c) * Q\n",
    "            retailer_max = p * sales - w * optimal_stock\n",
    "        elif self.contract_type == \"buyback\":\n",
    "            if b > w:\n",
    "                b = w\n",
    "            retailer_profit = p * sales - w * Q + b * leftovers\n",
    "            manufacturer_profit = (w - c) * Q - b * leftovers\n",
    "            retailer_max =p * sales - w * optimal_stock + b * leftovers\n",
    "        elif self.contract_type == \"revenue-sharing\":\n",
    "            max_revenue_share = p - w\n",
    "            if r > max_revenue_share:\n",
    "                r = max_revenue_share\n",
    "            retailer_profit = (p - r) * sales - w * Q\n",
    "            manufacturer_profit = (w - c) * Q + r * sales\n",
    "            retailer_max = (p - r) * sales - w * optimal_stock\n",
    "\n",
    "        self.current_round += 1\n",
    "        done = self.current_round >= self.max_rounds\n",
    "        return self.state, (manufacturer_profit, retailer_profit), done, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q-Learning Agent\n",
    "class QLearningAgent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        action_space,\n",
    "        learning_rate=0.05,\n",
    "        discount_factor=0.95,\n",
    "        epsilon=1.0,\n",
    "        epsilon_decay=0.002,\n",
    "        min_epsilon=0.00,\n",
    "    ):\n",
    "        self.action_space = action_space\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        self.epsilon = epsilon\n",
    "        self.initial_epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.min_epsilon = min_epsilon\n",
    "        self.q_table = defaultdict(float)\n",
    "\n",
    "\n",
    "        self.human_actions = []\n",
    "        self.opponent_action_history = []\n",
    "        self.behavior_change_threshold = 2\n",
    "\n",
    "    def _state_to_key(self, state):\n",
    "        return 0\n",
    "\n",
    "    def get_possible_actions(self):\n",
    "        if isinstance(self.action_space, spaces.Discrete):\n",
    "            return range(self.action_space.n)\n",
    "        elif isinstance(self.action_space, spaces.MultiDiscrete):\n",
    "            ranges = [range(n) for n in self.action_space.nvec]\n",
    "            return list(itertools.product(*ranges))\n",
    "\n",
    "    def get_frozen_action_manufacturer(self, state, opponent_traits=None):\n",
    "        \"\"\"\n",
    "        Always returns the same predefined action for the manufacturer.\n",
    "\n",
    "        Args:\n",
    "            state: The current state of the environment (unused in this function).\n",
    "            opponent_traits: Traits of the opponent (unused in this function).\n",
    "\n",
    "        Returns:\n",
    "            The same predefined action for every call.\n",
    "        \"\"\"\n",
    "        # Define the action you want the manufacturer to always take.\n",
    "        # This action must be valid for the manufacturer's action space.\n",
    "        if not hasattr(self, 'frozen_action1'):\n",
    "            self.frozen_action1 = self.action_space.sample()  # Set a default action once.\n",
    "        \n",
    "        return self.frozen_action1\n",
    "        \n",
    "    def get_frozen_action_retailer(self, state, opponent_traits=None):\n",
    "        \"\"\"\n",
    "        Always returns the same predefined action for the retailer.\n",
    "\n",
    "        Args:\n",
    "            state: The current state of the environment (unused in this function).\n",
    "            opponent_traits: Traits of the opponent (unused in this function).\n",
    "\n",
    "        Returns:\n",
    "            The same predefined action for every call.\n",
    "        \"\"\"\n",
    "        # Define the action you want the retailer to always take.\n",
    "        # This action must be valid for the retailer's action space.\n",
    "        if not hasattr(self, 'frozen_action2'):\n",
    "            self.frozen_action2 = self.action_space.sample()  # Set a default action once.\n",
    "        \n",
    "        return self.frozen_action2\n",
    "\n",
    "\n",
    "    def get_action(self, state):\n",
    "        \n",
    "        self.epsilon = max(\n",
    "            self.min_epsilon,\n",
    "            self.epsilon - self.epsilon_decay  # * (1.0 - (risk_aversion - 0.5))\n",
    "        )\n",
    "\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return self.action_space.sample() if not isinstance(self.action_space, spaces.MultiDiscrete) else tuple(self.action_space.sample())\n",
    "        else:\n",
    "            state_key = self._state_to_key(state)\n",
    "            possible_actions = self.get_possible_actions()\n",
    "            q_values = [self.q_table[(state_key, a)] for a in possible_actions]\n",
    "            max_q = max(q_values)\n",
    "            max_actions = [a for a, q in zip(possible_actions, q_values) if q == max_q]\n",
    "            return random.choice(max_actions)\n",
    "\n",
    "    def update_q_table(self, state, action, reward, next_state):\n",
    "        state_key = self._state_to_key(state)\n",
    "        next_state_key = self._state_to_key(next_state)\n",
    "        possible_actions = self.get_possible_actions()\n",
    "        max_next_q = max([self.q_table[(next_state_key, a)] for a in possible_actions], default=0)\n",
    "        # No reward shaping by traits, just regular Q-update\n",
    "        self.q_table[(state_key, action)] += self.learning_rate * (\n",
    "            reward + self.discount_factor * max_next_q - self.q_table[(state_key, action)]\n",
    "        )\n",
    "\n",
    "    def save_agent(self, filename):\n",
    "        agent_data = {\n",
    "            \"q_table\": self.q_table,\n",
    "            \"learning_rate\": self.learning_rate,\n",
    "            \"discount_factor\": self.discount_factor,\n",
    "            \"epsilon\": self.epsilon,\n",
    "            \"initial_epsilon\": self.initial_epsilon,\n",
    "            \"epsilon_decay\": self.epsilon_decay,\n",
    "            \"min_epsilon\": self.min_epsilon,\n",
    "        }\n",
    "        with open(filename, \"wb\") as f:\n",
    "            pickle.dump(agent_data, f)\n",
    "\n",
    "    def load_agent(self, filename):\n",
    "        with open(filename, \"rb\") as f:\n",
    "            agent_data = pickle.load(f)\n",
    "        self.q_table = agent_data[\"q_table\"]\n",
    "        self.learning_rate = agent_data[\"learning_rate\"]\n",
    "        self.discount_factor = agent_data[\"discount_factor\"]\n",
    "        self.epsilon = agent_data[\"epsilon\"]\n",
    "        self.initial_epsilon = agent_data[\"initial_epsilon\"]\n",
    "        self.epsilon_decay = agent_data[\"epsilon_decay\"]\n",
    "        self.min_epsilon = agent_data[\"min_epsilon\"]\n",
    "\n",
    "    def update_q_value(self, state, action, reward, next_state):\n",
    "        \"\"\"\n",
    "        Standard Q-Learning update.\n",
    "        \"\"\"\n",
    "        possible_actions = self.get_possible_actions()\n",
    "        next_qs = [self.q_table[(self._state_to_key(next_state), a)] for a in possible_actions]\n",
    "        max_next_q = max(next_qs) if next_qs else 0.0\n",
    "\n",
    "        old_q = self.q_table[(self._state_to_key(next_state), action)]\n",
    "        new_q = old_q + self.learning_rate * (reward + self.discount_factor * max_next_q - old_q)\n",
    "        self.q_table[(self._state_to_key(next_state), action)] = new_q\n",
    "\n",
    "    def decay_epsilon(self):\n",
    "        \"\"\"\n",
    "        Decay epsilon after each step if desired.\n",
    "        \"\"\"\n",
    "        self.epsilon = max(self.min_epsilon, self.epsilon - self.epsilon_decay)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/1 completed. Final manufacturer epsilon=1.0000, retailer epsilon=0.0000\n",
      "All actions logged to actions_log.csv\n"
     ]
    }
   ],
   "source": [
    "def train_from_scratch_each_episode(\n",
    "    num_episodes=100,\n",
    "    rounds_per_episode=1000,\n",
    "    contract_type=\"wholesale\",\n",
    "    expected_sales=expected_sales,\n",
    "    learning_rate=0.05,\n",
    "    discount_factor=0.95,\n",
    "    epsilon=1.0,\n",
    "    epsilon_decay=0.002,\n",
    "    min_epsilon=0.00,\n",
    "):\n",
    "    \"\"\"\n",
    "    For each episode:\n",
    "      - Create new environment\n",
    "      - Initialize new manufacturer & retailer agents with fresh Q-tables\n",
    "      - Run 1000 steps\n",
    "      - Track Q-values or store them for analysis\n",
    "      - ALSO log manufacturer & retailer actions each round to a CSV\n",
    "    \"\"\"\n",
    "    # We'll still store the Q-value evolution in 'q_value_history'\n",
    "    q_value_history = []\n",
    "\n",
    "    # NEW: We'll store (episode, round, state, action, reward, etc.) in a list of dicts\n",
    "    action_log = []\n",
    "\n",
    "    for ep in range(num_episodes):\n",
    "        # Fresh environment\n",
    "        env = SupplyChainEnv(contract_type=contract_type, expected_sales=expected_sales, maxRounds=rounds_per_episode)\n",
    "        state = env.reset()\n",
    "\n",
    "        # Fresh agents (both manufacturer & retailer)\n",
    "        manufacturer_agent = QLearningAgent(env.manufacturer_action_space, learning_rate, discount_factor, epsilon, epsilon_decay, min_epsilon)\n",
    "        retailer_agent = QLearningAgent(env.retailer_action_space, learning_rate, discount_factor, epsilon, epsilon_decay, min_epsilon)\n",
    "\n",
    "        q_tables_per_round = []\n",
    "\n",
    "        for round_i in range(rounds_per_episode):\n",
    "            # 1) Manufacturer picks an action (frozen)\n",
    "            #mfg_action = manufacturer_agent.get_action(state)\n",
    "            mfg_action = manufacturer_agent.get_frozen_action_manufacturer(state)\n",
    "            state_mfg = env.manufacturer_step(mfg_action)\n",
    "\n",
    "            # 2) Retailer picks an action \n",
    "            ret_action = retailer_agent.get_action(state_mfg)\n",
    "            next_state, (mfg_reward, ret_reward), done, _ = env.retailer_step(ret_action)\n",
    "\n",
    "            # 3) Update Q-values for both agents\n",
    "            manufacturer_agent.update_q_value(state, mfg_action, mfg_reward, next_state)\n",
    "            retailer_agent.update_q_value(state_mfg, ret_action, ret_reward, next_state)\n",
    "\n",
    "            # -- LOG THE ACTIONS & REWARDS --\n",
    "            # Convert arrays to lists if needed so they can be saved as CSV-friendly data\n",
    "            action_log.append({\n",
    "                \"Episode\": ep,\n",
    "                \"Round\": round_i,\n",
    "                \"CurrentState\": state.tolist() if isinstance(state, np.ndarray) else state,\n",
    "                \"ManufacturerAction\": mfg_action,\n",
    "                \"RetailerAction\": ret_action,\n",
    "                \"ManufacturerReward\": mfg_reward,\n",
    "                \"RetailerReward\": ret_reward,\n",
    "                \"NextState\": next_state.tolist() if isinstance(next_state, np.ndarray) else next_state,\n",
    "                # If you also want to record Epsilon:\n",
    "                \"ManufacturerEpsilon\": manufacturer_agent.epsilon,\n",
    "                \"RetailerEpsilon\": retailer_agent.epsilon\n",
    "            })\n",
    "\n",
    "            # 5) Move to next state\n",
    "            state = next_state\n",
    "\n",
    "            # Optional: store Q-tables for analysis\n",
    "            q_tables_per_round.append({\n",
    "                'round': round_i,\n",
    "                'manufacturer_q_table': dict(manufacturer_agent.q_table),\n",
    "                'retailer_q_table': dict(retailer_agent.q_table),\n",
    "            })\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        q_value_history.append(q_tables_per_round)\n",
    "        print(f\"Episode {ep+1}/{num_episodes} completed. \"\n",
    "              f\"Final manufacturer epsilon={manufacturer_agent.epsilon:.4f}, \"\n",
    "              f\"retailer epsilon={retailer_agent.epsilon:.4f}\")\n",
    "\n",
    "    # -- AFTER ALL EPISODES, WRITE THE LOG TO A CSV --\n",
    "    df_log = pd.DataFrame(action_log)\n",
    "    df_log.to_csv(\"actions_log.csv\", index=False)\n",
    "    print(\"All actions logged to actions_log.csv\")\n",
    "\n",
    "    return q_value_history\n",
    "\n",
    "q_value_evolution = train_from_scratch_each_episode(\n",
    "    num_episodes=1,            # how many episodes\n",
    "    rounds_per_episode=10000,   # how many rounds in each episode\n",
    "    contract_type=\"wholesale\",\n",
    "    expected_sales=expected_sales,\n",
    "    learning_rate=0.05,\n",
    "    discount_factor=0.9,\n",
    "    epsilon=1.0,\n",
    "    epsilon_decay=0.0002,\n",
    "    min_epsilon=0.00,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Run 1/1\n",
      "Summary results saved to summary_results.csv\n",
      "          Run    Episode                                      LastMfgAction  \\\n",
      "0           1          1                                                  6   \n",
      "1           1          2                                                  7   \n",
      "2           1          3                                                  1   \n",
      "3           1          4                                                  5   \n",
      "4           1          5                                                  0   \n",
      "5           1          6                                                  5   \n",
      "6           1          7                                                  4   \n",
      "7           1          8                                                  8   \n",
      "8           1          9                                                  1   \n",
      "9           1         10                                                  8   \n",
      "10          1         11                                                  2   \n",
      "11          1         12                                                  3   \n",
      "12          1         13                                                  2   \n",
      "13          1         14                                                  1   \n",
      "14          1         15                                                  3   \n",
      "15          1         16                                                  2   \n",
      "16          1         17                                                  1   \n",
      "17          1         18                                                  0   \n",
      "18          1         19                                                  3   \n",
      "19          1         20                                                  8   \n",
      "20          1         21                                                  7   \n",
      "21          1         22                                                  5   \n",
      "22          1         23                                                  1   \n",
      "23          1         24                                                  2   \n",
      "24          1         25                                                  7   \n",
      "25          1         26                                                  7   \n",
      "26          1         27                                                  4   \n",
      "27          1         28                                                  7   \n",
      "28          1         29                                                  4   \n",
      "29          1         30                                                  8   \n",
      "30          1         31                                                  1   \n",
      "31          1         32                                                  8   \n",
      "32          1         33                                                  8   \n",
      "33          1         34                                                  7   \n",
      "34          1         35                                                  1   \n",
      "35          1         36                                                  8   \n",
      "36          1         37                                                  8   \n",
      "37          1         38                                                  5   \n",
      "38          1         39                                                  4   \n",
      "39          1         40                                                  9   \n",
      "40          1         41                                                  6   \n",
      "41          1         42                                                  4   \n",
      "42          1         43                                                  0   \n",
      "43          1         44                                                  5   \n",
      "44          1         45                                                  4   \n",
      "45          1         46                                                  8   \n",
      "46          1         47                                                  1   \n",
      "47          1         48                                                  9   \n",
      "48          1         49                                                  1   \n",
      "49          1         50                                                  8   \n",
      "50  Frequency  Frequency  {6: 2, 7: 6, 1: 9, 5: 5, 0: 3, 4: 6, 8: 10, 2:...   \n",
      "\n",
      "                        LastRetAction  \n",
      "0                                   4  \n",
      "1                                   3  \n",
      "2                                   2  \n",
      "3                                   3  \n",
      "4                                   3  \n",
      "5                                   3  \n",
      "6                                   2  \n",
      "7                                   0  \n",
      "8                                   2  \n",
      "9                                   2  \n",
      "10                                  4  \n",
      "11                                  2  \n",
      "12                                  0  \n",
      "13                                  4  \n",
      "14                                  2  \n",
      "15                                  0  \n",
      "16                                  1  \n",
      "17                                  3  \n",
      "18                                  3  \n",
      "19                                  2  \n",
      "20                                  4  \n",
      "21                                  1  \n",
      "22                                  0  \n",
      "23                                  4  \n",
      "24                                  1  \n",
      "25                                  3  \n",
      "26                                  0  \n",
      "27                                  1  \n",
      "28                                  2  \n",
      "29                                  2  \n",
      "30                                  4  \n",
      "31                                  0  \n",
      "32                                  4  \n",
      "33                                  0  \n",
      "34                                  0  \n",
      "35                                  0  \n",
      "36                                  0  \n",
      "37                                  4  \n",
      "38                                  2  \n",
      "39                                  4  \n",
      "40                                  2  \n",
      "41                                  2  \n",
      "42                                  0  \n",
      "43                                  4  \n",
      "44                                  0  \n",
      "45                                  2  \n",
      "46                                  2  \n",
      "47                                  2  \n",
      "48                                  0  \n",
      "49                                  4  \n",
      "50  {4: 11, 3: 7, 2: 15, 0: 13, 1: 4}  \n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def train_and_track_results(\n",
    "    num_runs=1,  # Number of times to run the training\n",
    "    num_episodes=100,\n",
    "    rounds_per_episode=1000,\n",
    "    contract_type=\"wholesale\",\n",
    "    expected_sales=expected_sales,\n",
    "    learning_rate=0.05,\n",
    "    discount_factor=0.90,\n",
    "    epsilon=1.0,\n",
    "    epsilon_decay=0.001,\n",
    "    min_epsilon=0.00,\n",
    "):\n",
    "    \"\"\"\n",
    "    Runs the training multiple times with specified parameters, tracks results\n",
    "    (last actions, frequencies), and returns a summary DataFrame.\n",
    "    \"\"\"\n",
    "\n",
    "    all_runs_results = []\n",
    "\n",
    "    for run in range(num_runs):\n",
    "        print(f\"Starting Run {run + 1}/{num_runs}\")\n",
    "        episode_results = []\n",
    "\n",
    "        for ep in range(num_episodes):\n",
    "            env = SupplyChainEnv(contract_type=contract_type, expected_sales=expected_sales, maxRounds=rounds_per_episode)\n",
    "            state = env.reset()\n",
    "            manufacturer_agent = QLearningAgent(env.manufacturer_action_space)\n",
    "            retailer_agent = QLearningAgent(env.retailer_action_space)\n",
    "\n",
    "            last_mfg_action = None\n",
    "            last_ret_action = None\n",
    "\n",
    "            for round_i in range(rounds_per_episode):\n",
    "                mfg_action = manufacturer_agent.get_frozen_action_manufacturer(state)\n",
    "                state_mfg = env.manufacturer_step(mfg_action)\n",
    "                ret_action = retailer_agent.get_action(state_mfg)\n",
    "                next_state, _, done, _ = env.retailer_step(ret_action) # rewards not needed here\n",
    "\n",
    "                manufacturer_agent.update_q_value(state, mfg_action, 0, next_state) #Dummy reward\n",
    "                retailer_agent.update_q_value(state_mfg, ret_action, 0, next_state) #Dummy reward\n",
    "\n",
    "                last_mfg_action = mfg_action\n",
    "                last_ret_action = ret_action\n",
    "                state = next_state\n",
    "\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "            episode_results.append({\n",
    "                \"Run\": run + 1,\n",
    "                \"Episode\": ep + 1,\n",
    "                \"LastMfgAction\": last_mfg_action,\n",
    "                \"LastRetAction\": last_ret_action,\n",
    "            })\n",
    "            #print(f\"Run {run + 1}/{num_runs}, Episode {ep+1}/{num_episodes} completed.\")\n",
    "\n",
    "        all_runs_results.extend(episode_results)\n",
    "\n",
    "    df_results = pd.DataFrame(all_runs_results)\n",
    "\n",
    "    # Calculate frequencies of last actions across all episodes and runs.\n",
    "    mfg_action_counts = Counter(df_results[\"LastMfgAction\"])\n",
    "    ret_action_counts = Counter(df_results[\"LastRetAction\"])\n",
    "\n",
    "    frequency_data = {\n",
    "        \"Run\": \"Frequency\",\n",
    "        \"Episode\": \"Frequency\",\n",
    "        \"LastMfgAction\": dict(mfg_action_counts),\n",
    "        \"LastRetAction\": dict(ret_action_counts),\n",
    "    }\n",
    "\n",
    "    df_results = pd.concat([df_results, pd.DataFrame([frequency_data])], ignore_index=True)\n",
    "    df_results.to_csv(\"summary_results.csv\", index=False)\n",
    "    print(\"Summary results saved to summary_results.csv\")\n",
    "\n",
    "    return df_results\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "summary_df = train_and_track_results(\n",
    "    num_runs=1, # Run the training 3 times\n",
    "    num_episodes=50, # 5 episodes each run\n",
    "    rounds_per_episode=1010, # 100 rounds each episode\n",
    "    expected_sales=expected_sales\n",
    ")\n",
    "\n",
    "print(summary_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_average_q_values(q_value_evolution, episode_index=0, agent=\"manufacturer\"):\n",
    "    \"\"\"\n",
    "    Plots the average Q-value per round for the specified agent in a given episode.\n",
    "    agent can be 'manufacturer' or 'retailer'.\n",
    "    episode_index is which episode's data to plot.\n",
    "    \"\"\"\n",
    "    round_data = q_value_evolution[episode_index]  # This is the list of rounds for one episode\n",
    "    avg_q_values = []\n",
    "\n",
    "    for rd in round_data:\n",
    "        if agent == \"manufacturer\":\n",
    "            q_table = rd[\"manufacturer_q_table\"]\n",
    "        else:\n",
    "            q_table = rd[\"retailer_q_table\"]\n",
    "        \n",
    "        # If Q-table is empty, average is zero (or skip)\n",
    "        if len(q_table) == 0:\n",
    "            avg_q_values.append(0)\n",
    "            continue\n",
    "\n",
    "        # Compute average Q-value\n",
    "        sum_q = sum(q_table.values())\n",
    "        avg_q = sum_q / len(q_table)\n",
    "        avg_q_values.append(avg_q)\n",
    "\n",
    "    # Now we have a list of average Q-values, one per round\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(range(len(avg_q_values)), avg_q_values, label=f'{agent.capitalize()} Avg Q')\n",
    "    plt.xlabel(\"Round\")\n",
    "    plt.ylabel(\"Average Q-value\")\n",
    "    plt.title(f\"Episode {episode_index} - {agent.capitalize()} Average Q-value Over Rounds\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage: \n",
    "#   Plot average manufacturer Q for Episode 0.\n",
    "#   Then do the same for retailer or another episode if you like.\n",
    "\n",
    "\n",
    "\n",
    "#plot_average_q_values(q_value_evolution, episode_index=0, agent=\"manufacturer\")\n",
    "#plot_average_q_values(q_value_evolution, episode_index=0, agent=\"retailer\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
